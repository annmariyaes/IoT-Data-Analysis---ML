{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Text generation with an RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annmariyaes/IoT-Data-Analysis---ML/blob/main/Text_generation_with_an_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w21f4-ekBK4f"
      },
      "source": [
        "**Import TensorFlow and other libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKZX0bc78VUI"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYom3RoJ8dNd"
      },
      "source": [
        "**Download the Shakespeare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NHEzjRZ8hLL",
        "outputId": "06fbc6c4-462f-4eca-a152-4df6672b7b54"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('reddit_apple_android_2000.txt', 'https://raw.githubusercontent.com/minimaxir/textgenrnn/master/datasets/reddit_apple_android_2000.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/minimaxir/textgenrnn/master/datasets/reddit_apple_android_2000.txt\n",
            "155648/148087 [===============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXsYTsZ8k3I"
      },
      "source": [
        "**Read the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62RpVtau8lla",
        "outputId": "039aeeb1-23cf-4c5b-8a15-2a8f30def37a"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 147500 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRf1A1vD8qEv",
        "outputId": "f85d2795-33f9-438a-bb40-5d3f4d8037bd"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title\r\n",
            "The Apple Watch feature I once thought was a throwaway novelty is now crucial to me.\r\n",
            "Apple is trying to limit how often your iPhone apps can bug you to give them a rating\r\n",
            "The App Store now requires developers to use the official API to reque\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yLHcyP18tLb",
        "outputId": "c8100481-0a0e-4cde-f7ab-9769de736180"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0be9c-k8nmd"
      },
      "source": [
        "**Process the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsNlJzZV8w2O"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2MK4Mv-8_oT",
        "outputId": "1b9b1cb2-6a8e-499b-c545-8fbbe5c162d5"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  '#' :   5,\n",
            "  '$' :   6,\n",
            "  '%' :   7,\n",
            "  '&' :   8,\n",
            "  \"'\" :   9,\n",
            "  '(' :  10,\n",
            "  ')' :  11,\n",
            "  '*' :  12,\n",
            "  '+' :  13,\n",
            "  ',' :  14,\n",
            "  '-' :  15,\n",
            "  '.' :  16,\n",
            "  '/' :  17,\n",
            "  '0' :  18,\n",
            "  '1' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFPz51E79C-3",
        "outputId": "0eb2bd18-5a61-4af2-cc83-8a9618f5cf06"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'title\\r\\nThe Ap' ---- characters mapped to int ---- > [83 72 83 75 68  1  0 52 71 68  2 33 79]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhhZ9VL89F7Z"
      },
      "source": [
        "**Create training examples and targets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k71GVSlE9Gjm",
        "outputId": "bd855494-9ca2-44fe-cfd5-b6fcd8c99771"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t\n",
            "i\n",
            "t\n",
            "l\n",
            "e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xno9Txe09SZO",
        "outputId": "68fd6358-f293-45bb-bd38-68f7aa885b92"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'title\\r\\nThe Apple Watch feature I once thought was a throwaway novelty is now crucial to me.\\r\\nApple is'\n",
            "' trying to limit how often your iPhone apps can bug you to give them a rating\\r\\nThe App Store now requ'\n",
            "'ires developers to use the official API to request app ratings. Custom prompts are not allowed.\\r\\nI bu'\n",
            "'ilt a custom Reddit TouchBar interface!\\r\\nTIL that if Apple finds an underage worker in a factory of a'\n",
            "' supplier, They make the supplier return the child to their home, pay for their education, and pay fo'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch613oof9TXP"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI5lw7fo9Wzv",
        "outputId": "455482e1-9938-4f9f-f578-4685c8780fb8"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'title\\r\\nThe Apple Watch feature I once thought was a throwaway novelty is now crucial to me.\\r\\nApple i'\n",
            "Target data: 'itle\\r\\nThe Apple Watch feature I once thought was a throwaway novelty is now crucial to me.\\r\\nApple is'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYMjc4w19Xos",
        "outputId": "3dda2f4b-4606-465a-800d-acaedd3bb3a6"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 83 ('t')\n",
            "  expected output: 72 ('i')\n",
            "Step    1\n",
            "  input: 72 ('i')\n",
            "  expected output: 83 ('t')\n",
            "Step    2\n",
            "  input: 83 ('t')\n",
            "  expected output: 75 ('l')\n",
            "Step    3\n",
            "  input: 75 ('l')\n",
            "  expected output: 68 ('e')\n",
            "Step    4\n",
            "  input: 68 ('e')\n",
            "  expected output: 1 ('\\r')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srHpoMiD9diF"
      },
      "source": [
        "**Create training batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a898Hpqn9cP8",
        "outputId": "9f28845b-3037-4bf3-af26-99d2d9efe672"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibH3LjBg9kdh"
      },
      "source": [
        "**Build The Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xte_iUGu9muj"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Izn2xVL9osS"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWE9yP7_9pNu"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDlvIp9T9tcd"
      },
      "source": [
        "**Try the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfYRBeET9x4z",
        "outputId": "59db7626-0542-415b-f5d1-c58d8469d256"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 129) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkjrWKyv9z2A",
        "outputId": "a585a541-24f5-4843-a2c5-94780ca1b9f0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           33024     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 129)           132225    \n",
            "=================================================================\n",
            "Total params: 4,103,553\n",
            "Trainable params: 4,103,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eU725v93MN"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8ECXDnL941f",
        "outputId": "ac1f29d4-8cf2-4575-de1c-950aac71dad2"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 90,  88, 101,  24,  35,  47,  11,  84, 127,   7,  95,  30,  52,\n",
              "       114, 101, 105,  22,  63,  35,  26,  30,   3,   3, 128,  77, 125,\n",
              "        55,  92,  51, 116,  65,  10,  52,  35,  23,   3,  98,   1,  99,\n",
              "       104, 102,  60, 127,  84,  76,  93,  39,  42,  72,  79,  11,  13,\n",
              "        39,  27, 111, 100,  82,  87,   7,  21,  96, 105,  11, 118, 122,\n",
              "        71, 119,  46, 122,  89,   8,  53,  48,  49,  59,  72,  29, 102,\n",
              "        76,  89, 127, 128, 102,  46,   2,  80,  37, 110, 103,  20,  68,\n",
              "        80,  91, 110,  70,  11,  74, 127,  21, 101])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmG8Z5-z96mH",
        "outputId": "28ca22e4-e601-4d99-ccda-22bbd8e8d84b"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'ackup\\r\\nIn case you were wondering, The Galaxy S8’s Always-on home button won’t burn-in\\r\\nSamsung will'\n",
            "\n",
            "Next Char Predictions: \n",
            " '|y\\u200b6CO)u\\ufeff%£=T″\\u200b‘4`C8=!!�n㎃W\\x80S€b(TC5!è\\ré—‑\\\\\\ufeffum\\x93GJip)+G9\\u202aಠsx%3¯‘)⇧✔h≈N✔z&UPQ[i;‑mz\\ufeff�‑N qE…–2eq~…g)k\\ufeff3\\u200b'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ODndnth99hB"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsZyCe_r9-T3",
        "outputId": "9ed40ecd-8ff9-4665-bf29-ade41b1d366f"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 129)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.8596144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGFzWOO4-Agy"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN16ycBF-EzC"
      },
      "source": [
        "**Configure checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu_cksin-CgZ"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnp3_Xsb-JuC"
      },
      "source": [
        "**Execute the training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpVGice--OCt"
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cZILg4c-Pl4",
        "outputId": "f7d45491-39e7-4f71-be4c-e6bd93dbcff8"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 122s 6s/step - loss: 4.3772\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 125s 6s/step - loss: 3.2951\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 126s 6s/step - loss: 2.9516\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 111s 5s/step - loss: 2.7144\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 112s 5s/step - loss: 2.6042\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 111s 5s/step - loss: 2.5267\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 117s 5s/step - loss: 2.4596\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 118s 5s/step - loss: 2.3941\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 115s 5s/step - loss: 2.3213\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 120s 5s/step - loss: 2.2477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wybvqUla-SZe"
      },
      "source": [
        "**Generate the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cbsrOJhX-U_y",
        "outputId": "3cbddfbb-a0a5-4adf-de3e-bd5339178c54"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg6MRNTg-Wm2"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7_noI6y-YHx",
        "outputId": "5dbb003c-fdd5-49f6-a970-225dfb87db03"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            33024     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 129)            132225    \n",
            "=================================================================\n",
            "Total params: 4,103,553\n",
            "Trainable params: 4,103,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKckwoJT-k7G"
      },
      "source": [
        "**The Predication Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsGj8Idj-hEH"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAk6ly68-oR-",
        "outputId": "8abd4ab4-7851-4247-c2f9-580b6f772fb3"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: …ツ:5 Pleat mack: ishole ass fontert to customize Kodm'\r\n",
            "Ebersiging tol mocont\"'s usnet\r\n",
            "Apple Shentis\r\n",
            "Wherogbe with Appleald Pixtaled oring op redericaly RAgPle adr'tedat prain ifforele\r\n",
            "Scarie 5.33\r\n",
            "Apple Mcamterts: I -iznow Whith 834 2018 iftil\r\n",
            "Winde reanch than'(\"saner spoftchine thaby\r\n",
            "Googha Appronen's sAppleading uroms iOS 110 Nea jacked foo Anroid -ntiod Phaserm Andrigid\r\n",
            "isPhall Ganable of to ines, to icat\r\n",
            "Vish With Mong Uss Couries an S7 sigre reerounding, issung Starims GIcrook toy PrDe, ane inated Bang approifilistablation for is to Apple Mustages repped\r\n",
            "Androude hoved levine LiMfamp:\r\n",
            "The's Hor 27 riotur shortidays gow Apple sontcod 4h-gayee\r\n",
            "Reaplup unde's new ixel iPP. bo foured reweroit te perouccy infia cam juscass off 200ine excin a vinture in aly Sapplo des LEiPhopa Havew', ther chone wertatabe tiof jutin \".\".\r\n",
            "Ta is to 'neron in Banthrtu the sirfars op nethamed\r\n",
            "Google Plakevs coun cagas focksurem wable iat vrot is upsing\r\n",
            "OS rellengointado singet ressihnticllase\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIF1LdJ3-r0C"
      },
      "source": [
        "**Advanced: Customized Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63tqyPEt-tyx",
        "outputId": "1598f2fc-0700-442c-b80d-ec9d5d138e90"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OnqCn-D-xDa"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxQie98T-zcs"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inp)\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                target, predictions, from_logits=True))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWYjnxTU-1r6",
        "outputId": "3d65c394-0252-443c-a7a0-16e86f8af9ba"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # resetting the hidden state at the start of every epoch\n",
        "    model.reset_states()\n",
        "\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        loss = train_step(inp, target)\n",
        "\n",
        "        if batch_n % 100 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {}'\n",
        "            print(template.format(epoch + 1, batch_n, loss))\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.860799789428711\n",
            "Epoch 1 Loss 3.5946\n",
            "Time taken for 1 epoch 117.60727190971375 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.5842597484588623\n",
            "Epoch 2 Loss 3.1275\n",
            "Time taken for 1 epoch 115.94977259635925 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.090139150619507\n",
            "Epoch 3 Loss 2.8029\n",
            "Time taken for 1 epoch 116.42076849937439 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.814554452896118\n",
            "Epoch 4 Loss 2.6522\n",
            "Time taken for 1 epoch 118.30856108665466 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.660689353942871\n",
            "Epoch 5 Loss 2.5688\n",
            "Time taken for 1 epoch 113.33480215072632 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.5623369216918945\n",
            "Epoch 6 Loss 2.4719\n",
            "Time taken for 1 epoch 116.50770401954651 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.521909236907959\n",
            "Epoch 7 Loss 2.3924\n",
            "Time taken for 1 epoch 118.69168257713318 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.431535482406616\n",
            "Epoch 8 Loss 2.3977\n",
            "Time taken for 1 epoch 123.91789102554321 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.353118896484375\n",
            "Epoch 9 Loss 2.2645\n",
            "Time taken for 1 epoch 130.6286175251007 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.294604778289795\n",
            "Epoch 10 Loss 2.2212\n",
            "Time taken for 1 epoch 124.82714986801147 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Text generation with an RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w21f4-ekBK4f"
      },
      "source": [
        "**Import TensorFlow and other libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKZX0bc78VUI"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYom3RoJ8dNd"
      },
      "source": [
        "**Download the Shakespeare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NHEzjRZ8hLL",
        "outputId": "6f4c489b-1613-4cd2-85b1-4434e5a3bd87"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXsYTsZ8k3I"
      },
      "source": [
        "**Read the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62RpVtau8lla",
        "outputId": "39b2f23c-07b9-487f-e0cb-af8d9d45f7e6"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRf1A1vD8qEv",
        "outputId": "2d17b05d-5c0b-4749-d295-e71e9dfb6c76"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yLHcyP18tLb",
        "outputId": "c6a32cc5-ac22-4480-d7f6-aa9e9762d90e"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0be9c-k8nmd"
      },
      "source": [
        "**Process the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsNlJzZV8w2O"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2MK4Mv-8_oT",
        "outputId": "63bd3b35-4385-4e28-a285-06caa888c36b"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFPz51E79C-3",
        "outputId": "38841a15-e08e-4ca0-8284-bdbefd03d56f"
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhhZ9VL89F7Z"
      },
      "source": [
        "**Create training examples and targets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k71GVSlE9Gjm",
        "outputId": "9be6b51d-6a80-4532-bad6-390803f92f67"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xno9Txe09SZO",
        "outputId": "ec6c611e-dfed-4d44-f918-82c57801f7b8"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch613oof9TXP"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI5lw7fo9Wzv",
        "outputId": "7d3d7cb5-0a21-401e-f625-1fde715ce467"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYMjc4w19Xos",
        "outputId": "40b825bc-115e-46bf-8f06-8dd6a28ae0f7"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srHpoMiD9diF"
      },
      "source": [
        "**Create training batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a898Hpqn9cP8",
        "outputId": "c6f0ca37-b6b7-4d21-f3af-bb1a799ed5ea"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibH3LjBg9kdh"
      },
      "source": [
        "**Build The Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xte_iUGu9muj"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Izn2xVL9osS"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWE9yP7_9pNu"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDlvIp9T9tcd"
      },
      "source": [
        "**Try the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfYRBeET9x4z",
        "outputId": "7b25f07f-43f5-4164-9d54-bc768afec8b3"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkjrWKyv9z2A",
        "outputId": "038caff2-9563-4769-9f69-e5a54b555ec6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eU725v93MN"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8ECXDnL941f",
        "outputId": "d56da020-56aa-4c52-9692-cad1b14041c1"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([50,  2, 31,  6, 34, 30, 26, 13, 14,  8, 61, 17,  3, 28, 43, 10, 39,\n",
              "       25, 35, 53, 13,  6, 56, 29, 43,  9, 49, 33, 59, 39, 42, 37, 25, 53,\n",
              "       35, 48, 59, 33, 27, 48, 43, 31,  2, 58,  9,  7, 61, 63, 14, 55, 13,\n",
              "       21, 39, 63, 53, 59, 64, 42, 33, 35, 55, 50, 45, 56, 10, 60,  6,  9,\n",
              "       21,  4, 58, 53, 51, 35,  8, 51, 36, 21, 36, 35, 35, 28,  5,  5, 48,\n",
              "       23, 31, 31, 42, 59, 62,  4, 53, 36, 54, 38, 41, 39, 14, 63])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmG8Z5-z96mH",
        "outputId": "0eb13f64-1a5a-4835-eff0-88a2c4787238"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " ' tyrant;\\nAnd rather proved the sliding of your brother\\nA merriment than a vice.\\n\\nISABELLA:\\nO, pardon'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"l!S,VRNAB.wE$Pe:aMWoA,rQe3kUuadYMoWjuUOjeS!t3-wyBqAIayouzdUWqlgr:v,3I&tomW.mXIXWWP''jKSSdux&oXpZcaBy\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ODndnth99hB"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsZyCe_r9-T3",
        "outputId": "df0aac3d-789c-4170-dd6d-556444fe447b"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1733656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGFzWOO4-Agy"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN16ycBF-EzC"
      },
      "source": [
        "**Configure checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu_cksin-CgZ"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnp3_Xsb-JuC"
      },
      "source": [
        "**Execute the training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpVGice--OCt"
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cZILg4c-Pl4",
        "outputId": "0d61d1a9-ad5c-4f00-8b4a-57b6cfbb7ef0"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 880s 5s/step - loss: 2.6917\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 874s 5s/step - loss: 1.9596\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 886s 5s/step - loss: 1.6907\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 888s 5s/step - loss: 1.5420\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 882s 5s/step - loss: 1.4537\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 882s 5s/step - loss: 1.3934\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 886s 5s/step - loss: 1.3478\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 880s 5s/step - loss: 1.3085\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 874s 5s/step - loss: 1.2741\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 877s 5s/step - loss: 1.2410\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wybvqUla-SZe"
      },
      "source": [
        "**Generate the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cbsrOJhX-U_y",
        "outputId": "93305a80-e2fd-4172-adfa-27e083c32589"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg6MRNTg-Wm2"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7_noI6y-YHx",
        "outputId": "39e90590-70b2-4a82-e113-a44128e74eba"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKckwoJT-k7G"
      },
      "source": [
        "**The Predication Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsGj8Idj-hEH"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAk6ly68-oR-",
        "outputId": "369315a4-efad-40e0-af3f-dc60a7135fb3"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I cannot make gone;\n",
            "As mine are nursed, with in his body's pale,\n",
            "And now, outrage wrecked; 'tis away.\n",
            "I'll make a strange of some faults, sir, as you say do not fear.\n",
            "\n",
            "TRANIO:\n",
            "Sir I:\n",
            "That news as sweetaling sufferance to equit,\n",
            "Whome-maristops and riptles, those are enter'd:\n",
            "But what say thy father is my cell,\n",
            "Thou did charget knowledge spee like Henry.\n",
            "\n",
            "Lord:\n",
            "Never show begina witness, sir, dogns our gail.\n",
            "\n",
            "KING RICHARD III:\n",
            "Here conjusts pot thy hand: Comfort, moe befall, I pray?\n",
            "Go love, I'll one things, buy as a noble form.\n",
            "What pleasant he is a gentleman:\n",
            "Nay, but rempath you so so.\n",
            "\n",
            "POLIXENS:\n",
            "Not at your highness' lord guiler two\n",
            "Which thou shalt smocts our strange; disbreased it is:\n",
            "he suns me go with ame: to let him\n",
            "When they shall be free bless-tizens?\n",
            "\n",
            "CATESBY:\n",
            "\n",
            "Citizens:\n",
            "Who knows not the cetter I reseech you,\n",
            "Joind to the citcheth of this lowful head!\n",
            "\n",
            "QUEEN MARGARET:\n",
            "No measure, sir, wholks, at awly, you're put them\n",
            "To your life cares. Now, I beseech you, love, is he threa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIF1LdJ3-r0C"
      },
      "source": [
        "**Advanced: Customized Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63tqyPEt-tyx",
        "outputId": "3e130303-7ef2-493b-f081-9feab25842c7"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OnqCn-D-xDa"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxQie98T-zcs"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inp)\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                target, predictions, from_logits=True))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWYjnxTU-1r6",
        "outputId": "aa27f088-86c6-47fb-d4c6-931552f5aeed"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # resetting the hidden state at the start of every epoch\n",
        "    model.reset_states()\n",
        "\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        loss = train_step(inp, target)\n",
        "\n",
        "        if batch_n % 100 == 0:\n",
        "            template = 'Epoch {} Batch {} Loss {}'\n",
        "            print(template.format(epoch + 1, batch_n, loss))\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.174066543579102\n",
            "Epoch 1 Batch 100 Loss 2.3446543216705322\n",
            "Epoch 1 Loss 2.1371\n",
            "Time taken for 1 epoch 897.8744790554047 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1336426734924316\n",
            "Epoch 2 Batch 100 Loss 1.9290683269500732\n",
            "Epoch 2 Loss 1.7896\n",
            "Time taken for 1 epoch 877.5726189613342 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.751135230064392\n",
            "Epoch 3 Batch 100 Loss 1.673738718032837\n",
            "Epoch 3 Loss 1.5923\n",
            "Time taken for 1 epoch 892.8444237709045 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6366620063781738\n",
            "Epoch 4 Batch 100 Loss 1.514611840248108\n",
            "Epoch 4 Loss 1.4775\n",
            "Time taken for 1 epoch 883.0398263931274 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4380489587783813\n",
            "Epoch 5 Batch 100 Loss 1.4721332788467407\n",
            "Epoch 5 Loss 1.4421\n",
            "Time taken for 1 epoch 878.3726303577423 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3592214584350586\n",
            "Epoch 6 Batch 100 Loss 1.3682396411895752\n",
            "Epoch 6 Loss 1.4280\n",
            "Time taken for 1 epoch 873.6201405525208 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3047558069229126\n",
            "Epoch 7 Batch 100 Loss 1.370040774345398\n",
            "Epoch 7 Loss 1.3784\n",
            "Time taken for 1 epoch 875.5029513835907 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2281017303466797\n",
            "Epoch 8 Batch 100 Loss 1.3069225549697876\n",
            "Epoch 8 Loss 1.3158\n",
            "Time taken for 1 epoch 888.3429498672485 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2448604106903076\n",
            "Epoch 9 Batch 100 Loss 1.2310512065887451\n",
            "Epoch 9 Loss 1.2815\n",
            "Time taken for 1 epoch 894.4129903316498 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.221207857131958\n",
            "Epoch 10 Batch 100 Loss 1.2665350437164307\n",
            "Epoch 10 Loss 1.2425\n",
            "Time taken for 1 epoch 886.5137410163879 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}